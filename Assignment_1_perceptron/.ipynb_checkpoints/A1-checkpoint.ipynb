{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Assignment 1 of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 1: the perceptron\n",
    "### Task 1: Dataset Generation (unshuffled data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "mea=[(1,2),(1,1)]\n",
    "cov=[[[1,0],[0,1]],[[1,0],[0,1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def distribution_sampling(mea,cov):\n",
    "    gau_sam1 = np.random.multivariate_normal(mean=mea[0],cov=cov[0],size=100,check_valid='raise')\n",
    "    gau_sam2 = np.random.multivariate_normal(mean=mea[1],cov=cov[1],size=100,check_valid='raise')\n",
    "    label1=np.ones(100)\n",
    "    label2=np.ones(100)*-1\n",
    "    X_train1,X_test1,y_train1,y_test1 = train_test_split(gau_sam1,label1,test_size=0.2,random_state=42)\n",
    "    X_train2,X_test2,y_train2,y_test2 = train_test_split(gau_sam2,label2,test_size=0.2,random_state=18)\n",
    "    X_train=np.concatenate((X_train1,X_train2),axis=0)\n",
    "    X_test=np.concatenate((X_test1,X_test2),axis=0)\n",
    "    y_train=np.concatenate((y_train1,y_train2),axis=0)\n",
    "    y_test=np.concatenate((y_test1,y_test2),axis=0)\n",
    "    return X_train,X_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training points:  [[ 1.10318474  3.12505738]\n",
      " [ 1.95709931  3.15422105]\n",
      " [-0.5006382   1.7342948 ]]\n",
      "Training labels:  [1. 1. 1.]\n",
      "Testing points:  [[0.233832   2.46698568]\n",
      " [1.85401062 2.66896393]\n",
      " [0.27130446 2.28390751]]\n",
      "Testing labels:  [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "mean=[(1,2),(1,1)]\n",
    "cova=[[[1,0],[0,1]],[[1,0],[0,1]]]\n",
    "X_train,X_test,y_train,y_test = distribution_sampling(mean,cova)\n",
    "print(\"Training points: \",X_train[0:3,:])\n",
    "print(\"Training labels: \",y_train[0:3])\n",
    "print(\"Testing points: \",X_test[0:3,:])\n",
    "print(\"Testing labels: \",y_test[0:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 2: Perceptron Implementation\n",
    "### For detailed implementation, please refer to perceptron.py\n",
    "## Task 3: Train and Test on the perceptron model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  60.0 %\n",
      "Pred:  [ 1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1.  1.  1.  1.\n",
      "  1.  1.  1. -1. -1.  1. -1. -1. -1.  1. -1.  1.  1. -1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1.]\n",
      "Labels:  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "from Assignment1_perceptron import perceptron\n",
    "random_seed_pc = 40\n",
    "\n",
    "def perceptron_training(X_train,y_train,X_test,y_test):\n",
    "    pcp = perceptron.Perceptron(random_seed_pc)\n",
    "    pcp.train(X_train,y_train)\n",
    "    accuracy,pred = pcp.test(X_test,y_test)\n",
    "    print(\"Accuracy: \",accuracy*100,\"%\")\n",
    "    print(\"Pred: \",pred)\n",
    "    print(\"Labels: \",y_test)\n",
    "\n",
    "perceptron_training(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 4: Experiments with different settings of means and variance\n",
    "### Experiment 1: close means and high variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  57.49999999999999 %\n",
      "Pred:  [-1.  1. -1. -1. -1.  1.  1. -1. -1. -1. -1. -1.  1.  1.  1.  1. -1. -1.\n",
      "  1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1. -1.  1. -1.  1. -1.\n",
      " -1. -1. -1.  1.]\n",
      "Labels:  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "mean=[(1,2),(1,1)]\n",
    "cova=[[[60,0],[0,60]],[[50,0],[0,50]]]\n",
    "X_train,X_test,y_train,y_test = distribution_sampling(mean,cova)\n",
    "perceptron_training(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Experiment 2: close means and low variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  60.0 %\n",
      "Pred:  [ 1. -1.  1. -1. -1.  1. -1.  1.  1. -1. -1.  1.  1.  1. -1. -1.  1. -1.\n",
      "  1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1. -1. -1.  1.]\n",
      "Labels:  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "mean=[(1,2),(1,1)]\n",
    "cova=[[[1,0],[0,1]],[[2,0],[0,2]]]\n",
    "X_train,X_test,y_train,y_test = distribution_sampling(mean,cova)\n",
    "perceptron_training(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: distant means and high variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  100.0 %\n",
      "Pred:  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1.]\n",
      "Labels:  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "mean=[(1,2),(80,60)]\n",
    "cova=[[[60,0],[0,60]],[[50,0],[0,50]]]\n",
    "X_train,X_test,y_train,y_test = distribution_sampling(mean,cova)\n",
    "perceptron_training(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4: distant means and low variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  100.0 %\n",
      "Pred:  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1.]\n",
      "Labels:  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "mean=[(1,2),(80,60)]\n",
    "cova=[[[1,0],[0,1]],[[2,0],[0,2]]]\n",
    "X_train,X_test,y_train,y_test = distribution_sampling(mean,cova)\n",
    "perceptron_training(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### In conclusion, if the two gaussian distributions have distant means, the classification accuracy tends to increase.\n",
    "### If they have close means, the performance will go down a lot.\n",
    "### But the variance does not seem to cause any different.\n",
    "\n",
    "## Part 2: the multi-layer perceptron\n",
    "### Task 1: MLP architecture\n",
    "#### Please refer to mlp_numpy.py and modules.py\n",
    "### Task 2: training and testing implementation\n",
    "#### Please refer to the train_mlp_numpy.py\n",
    "### Task 3: Accuracy curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import train_mlp_numpy as tmn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "x,test_acc, train_acc = tmn.main()\n",
    "plt.plot(x,test_acc,'r--',label='test')\n",
    "plt.plot(x,train_acc,'g--',label='train')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs324",
   "language": "python",
   "name": "cs324"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
